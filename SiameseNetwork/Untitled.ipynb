{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Siamese network for CIFAR-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named keras",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-21449c3b25b2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0munpickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named keras"
     ]
    }
   ],
   "source": [
    "import cPickle\n",
    "import random\n",
    "import os\n",
    "import numpy as np\n",
    "import keras\n",
    "\n",
    "def unpickle(file):\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = cPickle.load(fo)\n",
    "    return dict\n",
    "\n",
    "def loadCIFAR10(size,labels,freq=0,val_split=0.2):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        SIZE:       size of the training data, including the validation set\n",
    "        labels:     which labels to use\n",
    "        freq:       relative frequency of the classes, if let to zero every class will\n",
    "                    appear equally often\n",
    "        val_split:  how much of the training data should be used for validation\n",
    "    Output:\n",
    "        (x_train,y_train,x_val,y_val,x_test,y_test)\n",
    "    \"\"\"\n",
    "\n",
    "    if(freq==0):\n",
    "        freq = [1./len(labels)] * len(labels)\n",
    "\n",
    "    fsum    = sum(freq)\n",
    "    NOL     = len(labels)\n",
    "\n",
    "    INPUTPATH = os.getcwd() + \"/../CIFAR-10\"\n",
    "\n",
    "    x_train = []\n",
    "    y_train = []\n",
    "\n",
    "    ##\n",
    "    ## Load data into x_train and y_train\n",
    "    ##\n",
    "\n",
    "    for f in range(1,6):\n",
    "        train = unpickle(INPUTPATH + \"/data_batch_\" + str(f))\n",
    "\n",
    "        x = train[\"data\"]\n",
    "        y = train[\"labels\"]\n",
    "\n",
    "        for i in range(len(y)):\n",
    "            if(len(y_train)==size*fsum):\n",
    "                break\n",
    "            if(y[i] in labels):\n",
    "                if(y_train.count(y[i])<=size*freq[labels.index(y[i])]):\n",
    "                    x_train.append(x[i])\n",
    "                    y_train.append(y[i])\n",
    "\n",
    "        if(len(y_train)==size*fsum):\n",
    "            break\n",
    "\n",
    "    ##\n",
    "    ## Bring data into the correct format\n",
    "    ##\n",
    "\n",
    "    x_train = np.array(x_train, float)\n",
    "    y_train = np.array(y_train, int)\n",
    "\n",
    "    x_train = np.reshape(x_train,(len(x_train),3,32,32))\n",
    "    x_train = np.transpose(x_train,(0,3,1,2))\n",
    "    x_train = np.transpose(x_train,(0,1,3,2))\n",
    "\n",
    "    x_train = x_train*2./255. - 1.\n",
    "\n",
    "\n",
    "    y_train = keras.utils.to_categorical(y_train, NOL)\n",
    "\n",
    "    ##\n",
    "    ## shuffle data in case the original data was in order, preventing\n",
    "    ## filling the validation set with only one class\n",
    "    ##\n",
    "\n",
    "    tmp = zip(x_train,y_train)\n",
    "    random.shuffle(tmp)\n",
    "\n",
    "    x_train,y_train = zip(*tmp)\n",
    "\n",
    "    ##\n",
    "    ## Split data into training set and validation set\n",
    "    ##\n",
    "\n",
    "    x_val = x_train[int(size*(1-val_split)):-1]\n",
    "    y_val = y_train[int(size*(1-val_split)):-1]\n",
    "    x_train = x_train[0:int(size*(1-val_split))]\n",
    "    y_train = y_train[0:int(size*(1-val_split))]\n",
    "\n",
    "    test = unpickle(INPUTPATH + \"/test_batch\")\n",
    "\n",
    "    x = test[\"data\"]\n",
    "    y = test[\"labels\"]\n",
    "\n",
    "    x_test = []\n",
    "    y_test = []\n",
    "\n",
    "    for i in range(len(y)):\n",
    "        if(len(y_test) == size*fsum/10.):\n",
    "            break\n",
    "        if(y[i] in labels):\n",
    "            if(y_test.count(y[i])<=size*freq[labels.index(y[i])]/10.):\n",
    "                x_test.append(x[i])\n",
    "                y_test.append(y[i])\n",
    "\n",
    "    x_test = np.array(x_test, float)\n",
    "    y_test = np.array(y_test, int)\n",
    "\n",
    "    x_test = np.reshape(x_test,(len(x_test),3,32,32))\n",
    "    x_test = np.transpose(x_test,(0,3,1,2))\n",
    "    x_test = np.transpose(x_test,(0,1,3,2))\n",
    "\n",
    "    x_test = x_test*2./255. - 1.\n",
    "\n",
    "    y_test = keras.utils.to_categorical(y_test, NOL)\n",
    "\n",
    "    return (x_train,y_train,x_val,y_val,x_test,y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
